from urllib.parse import urljoin
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from bs4 import BeautifulSoup

SITE = 'https://www.website.com/'
visited = []

class LinkChecker:
    def __init__(self, site):
        self.site = site
        self.visited = []

    def get_html(self, url):
        session = requests.Session()
        retry_strategy = Retry(
            total=5,  # Total number of retries
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"],  # Updated to use 'allowed_methods'
            backoff_factor=1  # Time between retries
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount('http://', adapter)
        session.mount('https://', adapter)

        try:
            response = session.get(url, timeout=20)  # Increased timeout
            response.raise_for_status()  # Will raise an HTTPError for bad responses
            return response.text
        except requests.exceptions.HTTPError as e:
            print(f"HTTP error occurred for {url}: {e}")
        except requests.exceptions.ConnectionError as e:
            print(f"Connection error occurred for {url}: {e}")
        except requests.exceptions.Timeout as e:
            print(f"Timeout occurred for {url}: {e}")
        except requests.exceptions.RequestException as e:
            print(f"Request exception occurred for {url}: {e}")
        return None

    def find_broken_links(self, url, parent=None):
        if url in self.visited:
            return
        self.visited.append(url)
        print('Checking URL:', url)
        html = self.get_html(url)
        if html is None:
            print(f"Could not retrieve {url}")
            return

        soup = BeautifulSoup(html, 'html.parser')
        links = soup.select('a[href]')
        for link in links:
            href = link['href']
            if href.startswith(('.', '#')):
                continue
            full_url = urljoin(url, href)
            if full_url.startswith(self.site) and full_url not in self.visited:
                self.find_broken_links(full_url, url)

if __name__ == '__main__':
    checker = LinkChecker(SITE)
    checker.find_broken_links(SITE)
