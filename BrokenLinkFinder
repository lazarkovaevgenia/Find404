from urllib.parse import urljoin
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import pandas as pd

SITE = 'https://www.website.com/'

class LinkChecker:
    def __init__(self, site):
        self.site = site
        self.visited = set()  # Changed to set for faster lookup
        self.broken_links = []

    def get_html(self, url):
        session = requests.Session()
        retry_strategy = Retry(
            total=5,  # Total number of retries
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"],  # Updated to use 'allowed_methods'
            backoff_factor=1  # Time between retries
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount('http://', adapter)
        session.mount('https://', adapter)

        try:
            response = session.get(url, timeout=20)  # Increased timeout
            response.raise_for_status()  # Will raise an HTTPError for bad responses
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"Error occurred for {url}: {e}")
            self.broken_links.append({'url': url, 'error': str(e)})
            return None

    def find_broken_links(self, url, parent=None):
        if url in self.visited:
            return
        self.visited.add(url)
        print('Checking URL:', url)
        html = self.get_html(url)
        if html is None:
            return

        soup = BeautifulSoup(html, 'html.parser')
        links = soup.select('a[href]')
        for link in links:
            href = link['href']
            if href.startswith(('.', '#')):
                continue
            full_url = urljoin(url, href)
            if full_url.startswith(self.site) and full_url not in self.visited:
                self.find_broken_links(full_url, url)

    def save_broken_links_to_excel(self):
        df = pd.DataFrame(self.broken_links)
        df.to_excel('broken_links.xlsx', index=False)

if __name__ == '__main__':
    checker = LinkChecker(SITE)
    checker.find_broken_links(SITE)
    checker.save_broken_links_to_excel()
